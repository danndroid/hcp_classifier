{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c80315-d33d-479c-8a59-1ddeef0a98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Dropout\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GATv2Conv, GCNConv, GlobalAttention\n",
    "from torch_geometric.utils import dropout_edge\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510b3a05-0118-4adb-a04d-24fd3f7d4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../../code/'))\n",
    "from utils.causal_graphs import create_graph_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea488eeb-8703-44a8-b465-064473ebf34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARRIOR\n",
      "   brain initialized\n",
      "   network initialized\n"
     ]
    }
   ],
   "source": [
    "import warrior as war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08bdccdc-de4f-4dc7-a30c-a8d958999628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 100, 100)\n",
      "(426, 100, 100)\n",
      "(426,)\n"
     ]
    }
   ],
   "source": [
    "causal_fcs = np.load('../local/causal_graphs/processed/causal_fcs.npy')\n",
    "print(causal_fcs.shape)\n",
    "fcs = np.load('../local/causal_graphs/processed/filtered_fcs.npy')\n",
    "print(fcs.shape)\n",
    "labels = np.load('../local/causal_graphs/processed/causal_labels.npy')\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6b567b-4e9d-44d3-8c17-ee72ac4b9240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_GRAPHS = fcs.shape[0]\n",
    "N_GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec72105c-989f-4f43-be52-da8e04c69332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 0.7488262910798122)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERCENT = 0.75 \n",
    "N_TRAINING_SAMPLES = int(np.floor(PERCENT*N_GRAPHS))\n",
    "N_TRAINING_SAMPLES, N_TRAINING_SAMPLES/N_GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b8dc134-2b92-42fb-be06-1080918d4e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "root = '../local/graph_datasets/hcp_causal/'\n",
    "dataset = create_graph_dataset(causal_fcs=causal_fcs, sparse_fcs=fcs, root=root, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e32ac0-86e1-4c76-9d68-a4809c7b146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: HCPCausalDataset(426):\n",
      "====================\n",
      "Number of graphs: 426\n",
      "Number of features: 9\n",
      "Number of classes: 2\n",
      "\n",
      "First Graph: Data(x=[100, 9], edge_index=[2, 1377], edge_attr=[1377], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 100\n",
      "Number of edges: 1377\n",
      "Average node degree: 13.77\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "graph = dataset[0]\n",
    "print()\n",
    "print(f'First Graph: {graph}')\n",
    "print('=============================================================')\n",
    "print(f'Number of nodes: {graph.num_nodes}')\n",
    "print(f'Number of edges: {graph.num_edges}')\n",
    "print(f'Average node degree: {graph.num_edges / graph.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {graph.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {graph.has_self_loops()}')\n",
    "print(f'Is undirected: {graph.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f8adc42-6dfa-426c-a090-af0becfcbac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13. ,  8. ,  5. ,  2.4,  2.9, -0.6,  0.1,  0.1,  0.1],\n",
       "       [10. ,  4. ,  6. , -0.1,  0.8, -1. ,  0.1,  0.1,  0.2],\n",
       "       [ 5. ,  3. ,  2. ,  1.6,  2. , -0.4,  0.1,  0. ,  0.2]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(graph.x[:3,:].numpy(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76355d18-945d-4cef-a5f9-92bb85fb44fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:223 M:203 - 0.910\n"
     ]
    }
   ],
   "source": [
    "num_pos = sum(1 for data in dataset if data.y == 1)\n",
    "num_neg = len(dataset) - num_pos\n",
    "print(f'F:{num_pos} M:{num_neg} - {num_neg/num_pos:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07b4addc-4e59-40c4-9b39-1f8ed6e8748d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 319\n",
      "Number of test graphs: 107\n"
     ]
    }
   ],
   "source": [
    "dataset_shuffled = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset_shuffled[:N_TRAINING_SAMPLES]\n",
    "test_dataset = dataset_shuffled[N_TRAINING_SAMPLES:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a130b6-95ee-407d-a520-376006a98cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)#, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)#, shuffle=False)\n",
    "# for step, data in enumerate(train_loader):\n",
    "#     print(f'Step {step + 1}:')\n",
    "#     print('=======')\n",
    "#     print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "#     print(data)\n",
    "#     print(f'Graphs to aggregate: {len(np.unique(data.batch))}')\n",
    "#     print(np.unique(data.batch))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42abe45c-9a92-4189-9253-c25a8388f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionGNN(nn.Module):\n",
    "    def __init__(self, hidden_channels, num_heads=1, dropout=0.5):\n",
    "        super(AttentionGNN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels) # SAGEConv, GATConv, GATv2Conv, GCNConv\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout1 = torch.nn.Dropout(0.6)\n",
    "        \n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)#, dropout=dropout)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout2 = torch.nn.Dropout(0.7)\n",
    "\n",
    "        #self.edge_drop = 0.3\n",
    "        \n",
    "        \n",
    "        # Attention-based graph-level aggregation\n",
    "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_channels * num_heads, 1))\n",
    "        nn.init.xavier_uniform_(self.attention_weights)  # Initialize attention weight\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = nn.Linear(hidden_channels * num_heads, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "\n",
    "        #edge_index, _ = dropout_edge(edge_index, p=self.edge_drop, training=self.training)\n",
    "\n",
    "        node_embeddings = self.conv1(x, edge_index)\n",
    "        node_embeddings = self.bn1(node_embeddings)\n",
    "        node_embeddings = self.relu1(node_embeddings)\n",
    "        node_embeddings = self.dropout1(node_embeddings)\n",
    "\n",
    "        node_embeddings = self.conv2(node_embeddings, edge_index)\n",
    "        node_embeddings = self.bn2(node_embeddings)\n",
    "        node_embeddings = self.relu2(node_embeddings)\n",
    "        node_embeddings = self.dropout2(node_embeddings)\n",
    "\n",
    "        # Step 2: Compute attention scores for each node in the graph\n",
    "        attention_scores = torch.matmul(node_embeddings, self.attention_weights)  # (num_nodes, 1)\n",
    "        attention_scores = torch.sigmoid(attention_scores)  # Normalize scores to [0,1]\n",
    "        \n",
    "        # Step 3: Compute graph embedding as weighted sum of node embeddings\n",
    "        weighted_embeddings = node_embeddings * attention_scores  # (num_nodes, hidden_dim * num_heads)\n",
    "        graph_embedding = global_add_pool(weighted_embeddings, batch)  # (num_graphs, hidden_dim * num_heads)\n",
    "\n",
    "        # Step 4: Compute logits for binary classification\n",
    "        logits = self.classifier(graph_embedding)  # (num_graphs, 1)\n",
    "        \n",
    "        return logits, attention_scores  # Return logits and node attention scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c09a49-489e-4748-a382-a548cdab2726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         data = data.to(device)\n",
    "         optimizer.zero_grad()\n",
    "         #print(f'Graphs in batch: {data.num_graphs}') \n",
    "         #print(f'data: {data}')\n",
    "         y_logits,_ = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         #print(y_logits.shape)\n",
    "        \n",
    "         #print(f'y_hat:{out.shape}')\n",
    "         #print(out)\n",
    "         y_logits = y_logits.to(device)\n",
    "         loss = criterion(y_logits.float(), torch.unsqueeze(data.y.float().to(device),1))  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # # Prevent exploding gradients by clipping\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         #optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         data = data.to(device)\n",
    "         y_logits,_ = model(data.x, data.edge_index, data.batch)\n",
    "         y_probs = torch.sigmoid(y_logits)\n",
    "         y_pred = (y_probs > 0.5).float()\n",
    "         y_pred = y_pred.to(device)\n",
    "         correct += int((y_pred.squeeze() == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d58e3e7-75f4-465b-8cb5-5ce92c843de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   4,   8,  16,  32,  64, 128, 256, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46a83b28-e970-43dc-ab11-5cb6e5778016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: 000, Train Acc: 0.4984, Test Acc: 0.5888\n",
      "Epoch: 250, Train Acc: 0.6520, Test Acc: 0.5607\n",
      "Epoch: 500, Train Acc: 0.6364, Test Acc: 0.4953\n",
      "Epoch: 750, Train Acc: 0.6646, Test Acc: 0.5794\n",
      "Epoch: 1000, Train Acc: 0.6458, Test Acc: 0.4953\n",
      "Epoch: 1250, Train Acc: 0.7774, Test Acc: 0.5701\n",
      "Epoch: 1500, Train Acc: 0.7304, Test Acc: 0.5514\n",
      "Epoch: 1750, Train Acc: 0.7492, Test Acc: 0.5888\n",
      "Epoch: 2000, Train Acc: 0.7429, Test Acc: 0.6542\n",
      "Epoch: 2250, Train Acc: 0.7304, Test Acc: 0.5327\n",
      "Epoch: 2500, Train Acc: 0.7555, Test Acc: 0.6262\n",
      "Epoch: 2750, Train Acc: 0.7743, Test Acc: 0.5421\n",
      "Epoch: 3000, Train Acc: 0.6426, Test Acc: 0.5140\n",
      "Epoch: 3250, Train Acc: 0.7304, Test Acc: 0.5327\n",
      "Epoch: 3500, Train Acc: 0.7241, Test Acc: 0.4953\n",
      "Epoch: 3750, Train Acc: 0.7618, Test Acc: 0.4673\n",
      "Epoch: 4000, Train Acc: 0.8433, Test Acc: 0.5888\n",
      "Epoch: 4250, Train Acc: 0.6646, Test Acc: 0.6168\n",
      "Epoch: 4500, Train Acc: 0.7179, Test Acc: 0.6262\n",
      "Epoch: 4750, Train Acc: 0.6803, Test Acc: 0.6355\n",
      "Epoch: 5000, Train Acc: 0.8150, Test Acc: 0.5047\n",
      "Epoch: 5250, Train Acc: 0.7335, Test Acc: 0.6355\n",
      "Epoch: 5500, Train Acc: 0.6740, Test Acc: 0.5514\n",
      "Epoch: 5750, Train Acc: 0.8495, Test Acc: 0.6075\n",
      "Epoch: 6000, Train Acc: 0.7994, Test Acc: 0.5794\n",
      "Epoch: 6250, Train Acc: 0.7241, Test Acc: 0.6822\n",
      "Epoch: 6500, Train Acc: 0.8589, Test Acc: 0.6542\n",
      "Epoch: 6750, Train Acc: 0.8495, Test Acc: 0.6355\n",
      "Epoch: 7000, Train Acc: 0.8150, Test Acc: 0.6168\n",
      "Epoch: 7250, Train Acc: 0.8464, Test Acc: 0.6262\n",
      "Epoch: 7500, Train Acc: 0.8558, Test Acc: 0.5607\n",
      "Epoch: 7750, Train Acc: 0.8715, Test Acc: 0.5794\n",
      "Epoch: 8000, Train Acc: 0.8495, Test Acc: 0.5701\n",
      "Epoch: 8250, Train Acc: 0.8401, Test Acc: 0.5047\n",
      "Epoch: 8500, Train Acc: 0.9028, Test Acc: 0.5701\n",
      "Epoch: 8750, Train Acc: 0.8370, Test Acc: 0.6355\n",
      "Epoch: 9000, Train Acc: 0.9091, Test Acc: 0.5981\n",
      "Epoch: 9250, Train Acc: 0.8621, Test Acc: 0.5701\n",
      "Epoch: 9500, Train Acc: 0.8871, Test Acc: 0.6355\n",
      "Epoch: 9750, Train Acc: 0.8621, Test Acc: 0.5701\n",
      "Epoch: 10000, Train Acc: 0.8589, Test Acc: 0.5981\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10001\n",
    "metrics = np.empty((2,EPOCHS))\n",
    "\n",
    "model = AttentionGNN(hidden_channels=128)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=0.0005,  weight_decay=1e-4, amsgrad=True)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    metrics[0,epoch] = train_acc\n",
    "    metrics[1,epoch] = test_acc\n",
    "\n",
    "    if epoch % 250 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8649f33-097d-45bb-a769-eb2a6221dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.shape)\n",
    "df = pd.DataFrame({'epoch': range(EPOCHS)[0::100], 'train': metrics[0,:][0::100], 'test': metrics[1,:][0::100]})\n",
    "df_melted = df.melt(id_vars='epoch', var_name='series', value_name='accuracy')\n",
    "sns.lineplot(x='epoch', y='accuracy', hue='series', data=df_melted)\n",
    "plt.plot([0,EPOCHS],[0.5,0.5], c='r', linestyle='--')\n",
    "plt.plot([0,EPOCHS],[0.6,0.6], c='gray', linestyle='--')\n",
    "plt.plot([0,EPOCHS],[0.7,0.7], c='gray', linestyle='--')\n",
    "plt.plot([0,EPOCHS],[0.8,0.8], c='gray', linestyle='--')\n",
    "plt.xlim(0,EPOCHS)\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e69fb9-9b40-40ac-ac92-6a4e3892fa5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04650e-7952-4d34-967e-d985370dc081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
